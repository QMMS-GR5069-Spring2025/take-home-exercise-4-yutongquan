{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db5479d3-d9af-4abd-8da2-e0f821d72ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Homework #5: Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d933fa-d81f-4d53-bcca-a8d1e90f0836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74911f9b-9dfc-4a3a-ad69-8e497e2e5a26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Establish a connection to the MySQL server\n",
    "conn = mysql.connector.connect(\n",
    "    host='yq2397-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com',\n",
    "    user='admin',\n",
    "    password='Corrine030212'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS gr5069\")\n",
    "cursor.execute(\"USE gr5069\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c49d9c-db0d-481f-ac26-ca06bac2825d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the first table for Model 1 predictions\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS model1_predictions (\n",
    "    prediction_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    race_id INT,\n",
    "    driver_id INT,\n",
    "    actual_position INT,\n",
    "    predicted_position FLOAT,\n",
    "    prediction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Create the second table for Model 2 predictions\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS model2_predictions (\n",
    "    prediction_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    race_id INT,\n",
    "    driver_id INT,\n",
    "    actual_position INT,\n",
    "    predicted_position FLOAT,\n",
    "    prediction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Verify tables were created\n",
    "cursor.execute(\"SHOW TABLES\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd0108a-9a90-4e56-904a-3a219dc15785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acfe8836-3c2d-448d-b84d-eb371c9c727b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up a new MLflow experiment\n",
    "experiment_name = f\"/Users/yq2397@columbia.edu/take-home-exercise-4\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3151c6c7-87cc-43b7-ab77-bd63342f5ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load F1 datasets from AWS S3\n",
    "df_results = spark.read.csv('s3://columbia-gr5069-main/raw/results.csv', header = True)\n",
    "df_drivers = spark.read.csv('s3://columbia-gr5069-main/raw/drivers.csv', header = True)\n",
    "df_races= spark.read.csv('s3://columbia-gr5069-main/raw/races.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d2622f-93d0-4462-b3af-5789e1a1c7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "# Merge the datasets\n",
    "merged_df = df_results.join(df_races, on=\"raceId\", how=\"left\")\n",
    "merged_df = merged_df.join(df_drivers, on=\"driverId\", how=\"left\")\n",
    "\n",
    "# Convert data types\n",
    "merged_df = merged_df.withColumn(\"grid\", merged_df[\"grid\"].cast(DoubleType()))\n",
    "merged_df = merged_df.withColumn(\"positionOrder\", merged_df[\"positionOrder\"].cast(DoubleType()))\n",
    "merged_df = merged_df.withColumn(\"laps\", merged_df[\"laps\"].cast(DoubleType()))\n",
    "\n",
    "# Calculate driver age\n",
    "merged_df = merged_df.withColumn(\"dob\", F.to_date(merged_df[\"dob\"]))\n",
    "merged_df = merged_df.withColumn(\"date\", F.to_date(merged_df[\"date\"]))\n",
    "merged_df = merged_df.withColumn(\"driver_age\", F.datediff(merged_df[\"date\"], merged_df[\"dob\"])/365.25)\n",
    "\n",
    "# Keep important columns\n",
    "model_df = merged_df.select(\"raceId\", \"driverId\", \"grid\", \"driver_age\", \"laps\", \"positionOrder\")\n",
    "model_df = model_df.dropna()\n",
    "\n",
    "print(f\"Total records for modeling: {model_df.count()}\")\n",
    "display(model_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deeab359-dbfb-4fd9-9c6d-e9a571f4caf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "train_df, test_df = model_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set count: {train_df.count()}\")\n",
    "print(f\"Testing set count: {test_df.count()}\")\n",
    "\n",
    "# Create feature vector\n",
    "feature_cols = [\"grid\", \"driver_age\", \"laps\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f188626c-449e-4cfb-b4d7-805be849adce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to train model and log in MLflow\n",
    "def train_and_log_model(model_type, params=None):\n",
    "    \"\"\"Train a model, log in MLflow, and return predictions\"\"\"\n",
    "    with mlflow.start_run() as run:\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        if params:\n",
    "            for key, value in params.items():\n",
    "                mlflow.log_param(key, value)\n",
    "        \n",
    "        # Prepare data\n",
    "        train_vector = assembler.transform(train_df)\n",
    "        test_vector = assembler.transform(test_df)\n",
    "        \n",
    "        # Create and train model\n",
    "        if model_type == 'rf':\n",
    "            model = RandomForestRegressor(featuresCol=\"features\", labelCol=\"positionOrder\", **params if params else {})\n",
    "        elif model_type == 'gbt':\n",
    "            model = GBTRegressor(featuresCol=\"features\", labelCol=\"positionOrder\", **params if params else {})\n",
    "        \n",
    "        trained_model = model.fit(train_vector)\n",
    "        predictions = trained_model.transform(test_vector)\n",
    "        \n",
    "        # Calculate and log metrics\n",
    "        evaluator = RegressionEvaluator(labelCol=\"positionOrder\", predictionCol=\"prediction\")\n",
    "        rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "        r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "        mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        pred_pd = predictions.select(\"positionOrder\", \"prediction\").toPandas()\n",
    "        accuracy_within_1 = np.mean(np.abs(pred_pd[\"positionOrder\"] - np.round(pred_pd[\"prediction\"])) <= 1) * 100\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"accuracy_within_1\", accuracy_within_1)\n",
    "        \n",
    "        # First artifact: Actual vs Predicted plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(pred_pd[\"positionOrder\"], pred_pd[\"prediction\"], alpha=0.5)\n",
    "        plt.plot([pred_pd[\"positionOrder\"].min(), pred_pd[\"positionOrder\"].max()], \n",
    "                 [pred_pd[\"positionOrder\"].min(), pred_pd[\"positionOrder\"].max()], 'r--')\n",
    "        plt.xlabel('Actual Position')\n",
    "        plt.ylabel('Predicted Position')\n",
    "        plt.title('Actual vs Predicted Race Positions')\n",
    "        plt.savefig(\"prediction_scatter.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Second artifact: Feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Importance': trained_model.featureImportances.toArray()\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.savefig(\"feature_importance.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Log artifacts\n",
    "        mlflow.log_artifact(\"prediction_scatter.png\")\n",
    "        mlflow.log_artifact(\"feature_importance.png\")\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.spark.log_model(trained_model, f\"{model_type}_model\")\n",
    "        \n",
    "        print(f\"Model: {model_type}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbc9281f-95ce-4af1-8cc5-75aa241adc4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train two models\n",
    "print(\"Building Model 1: Random Forest\")\n",
    "rf_params = {'numTrees': 200, 'maxDepth': 10, 'seed': 42}\n",
    "rf_predictions = train_and_log_model('rf', rf_params)\n",
    "\n",
    "print(\"\\nBuilding Model 2: Gradient Boosting\")\n",
    "gbt_params = {'maxIter': 300, 'stepSize': 0.05, 'maxDepth': 3, 'seed': 42}\n",
    "gbt_predictions = train_and_log_model('gbt', gbt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6330e0a7-9fbf-4e26-b8dc-802b35581dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to store predictions in the MySQL database\n",
    "def store_predictions_to_mysql(predictions_df, table_name):\n",
    "    \"\"\"Store model predictions in the specified database table\"\"\"\n",
    "    \n",
    "    # Convert predictions to a pandas DataFrame first\n",
    "    pandas_df = predictions_df.select(\n",
    "        \"raceId\", \n",
    "        \"driverId\", \n",
    "        F.col(\"positionOrder\").cast(\"int\").alias(\"actual_position\"), \n",
    "        F.round(\"prediction\", 2).alias(\"predicted_position\")\n",
    "    ).toPandas()\n",
    "    \n",
    "    # Connect to MySQL\n",
    "    conn = mysql.connector.connect(\n",
    "        host='yq2397-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com',\n",
    "        user='admin',\n",
    "        password='Corrine030212',\n",
    "        database='gr5069'\n",
    "    )\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Insert predictions\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO {table_name} (race_id, driver_id, actual_position, predicted_position)\n",
    "    VALUES (%s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    \n",
    "    rows_to_insert = []\n",
    "    for _, row in pandas_df.iterrows():\n",
    "        rows_to_insert.append((\n",
    "            int(row[\"raceId\"]),\n",
    "            int(row[\"driverId\"]),\n",
    "            int(row[\"actual_position\"]),\n",
    "            float(row[\"predicted_position\"])\n",
    "        ))\n",
    "    \n",
    "    cursor.executemany(insert_query, rows_to_insert)\n",
    "    conn.commit()\n",
    "    \n",
    "    # Verify insertion\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    row_count = cursor.fetchone()[0]\n",
    "    \n",
    "    print(f\"Successfully stored {len(rows_to_insert)} predictions in {table_name}\")\n",
    "    print(f\"Total rows in {table_name}: {row_count}\")\n",
    "    \n",
    "    # Close connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Store predictions from both models\n",
    "print(\"Storing Random Forest predictions...\")\n",
    "store_predictions_to_mysql(rf_predictions, \"model1_predictions\")\n",
    "\n",
    "print(\"\\nStoring Gradient Boosting predictions...\")\n",
    "store_predictions_to_mysql(gbt_predictions, \"model2_predictions\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "take-home-exercise-4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
